{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\"><b><span style=\"font-weight:bold; color:green\">EMR Spark</span> cluster with <span style=\"font-weight:bold; color:green\">Jupyter</span></b></div><hr>\n",
    "<div style=\"text-align:right;\">Sergei Yu. Papulin <span style=\"font-style: italic;font-weight: bold;\">(papulin_bmstu@mail.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "<div><span style=\"font-size:14pt; font-weight:bold\">Content</span>\n",
    "    <ol>\n",
    "        <li><a href=\"#1\">Deploying EMR Spark Cluster with Jupyter</a></li>\n",
    "        <li><a href=\"#2\">References</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Launch the cell below to apply a jupyter notebook style</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">1. Deploying EMR Spark Cluster with Jupyter </div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To Content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Display available subnets and pick an id of a subnet with 10.0.1.0/28</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws ec2 describe-subnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To install Jupyter, upload to S3 the following script</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"msg-block msg-ref\">\n",
    "      <div class=\"msg-text-ref\">\n",
    "          <p>The source of the bash code and guideline for deploying jupyter notebook in EMR cluster is <a href=\"https://bytes.babbel.com/en/articles/2017-07-04-spark-with-jupyter-inside-vpc.html\">here</a>. The below script has minor changes in comparison with the original.\n",
    "     </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /YOUR_PATH/config/install-jupyter.sh\n",
    "#!/usr/bin/env bash\n",
    "set -x -e\n",
    "\n",
    "JUPYTER_PASSWORD=${1:-\"myJupyterPassword\"}\n",
    "NOTEBOOK_DIR=${2:-\"s3://myS3Bucket/notebooks/\"}\n",
    "\n",
    "# home backup\n",
    "if [ ! -d /mnt/home_backup ]; then\n",
    "  sudo mkdir /mnt/home_backup\n",
    "  sudo cp -a /home/* /mnt/home_backup\n",
    "fi\n",
    "\n",
    "# mount home to /mnt\n",
    "if [ ! -d /mnt/home ]; then\n",
    "  sudo mv /home/ /mnt/\n",
    "  sudo ln -s /mnt/home /home\n",
    "fi\n",
    "\n",
    "# Install conda\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-4.2.12-Linux-x86_64.sh -O /home/hadoop/miniconda.sh \\\n",
    "    && /bin/bash ~/miniconda.sh -b -p $HOME/conda\n",
    "\n",
    "echo '\\nexport PATH=$HOME/conda/bin:$PATH' >> $HOME/.bashrc && source $HOME/.bashrc\n",
    "\n",
    "conda config --set always_yes yes --set changeps1 no\n",
    "\n",
    "conda install conda=4.2.13\n",
    "\n",
    "conda config -f --add channels conda-forge\n",
    "conda config -f --add channels defaults\n",
    "\n",
    "conda install hdfs3 findspark ujson jsonschema toolz boto3 py4j numpy pandas==0.19.2\n",
    "\n",
    "# cleanup\n",
    "rm ~/miniconda.sh\n",
    "\n",
    "echo bootstrap_conda.sh completed. PATH now: $PATH\n",
    "export PYSPARK_PYTHON=\"/home/hadoop/conda/bin/python3.5\"\n",
    "\n",
    "############### -------------- master node -------------- ###############\n",
    "\n",
    "IS_MASTER=false\n",
    "if grep isMaster /mnt/var/lib/info/instance.json | grep true;\n",
    "then\n",
    "  IS_MASTER=true\n",
    "\n",
    "  ### install dependencies for s3fs-fuse to access and store notebooks\n",
    "\n",
    "  sudo yum install -y git\n",
    "  sudo yum install -y libcurl libcurl-devel graphviz cyrus-sasl cyrus-sasl-devel readline readline-devel gnuplot\n",
    "  #sudo yum install -y automake fuse fuse-devel libxml2-devel\n",
    "\n",
    "  sudo yum install -y automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel\n",
    "  wget ftp://mirror.switch.ch/pool/4/mirror/epel/6/x86_64/Packages/j/jsoncpp-devel-0.10.5-2.el6.x86_64.rpm\n",
    "  wget ftp://mirror.switch.ch/pool/4/mirror/epel/6/x86_64/Packages/j/jsoncpp-0.10.5-2.el6.x86_64.rpm\n",
    "  sudo rpm -ivh *.rpm\n",
    "\n",
    "  # extract BUCKET and FOLDER to mount from NOTEBOOK_DIR\n",
    "  NOTEBOOK_DIR=\"${NOTEBOOK_DIR%/}/\"\n",
    "  BUCKET=$(python -c \"print('$NOTEBOOK_DIR'.split('//')[1].split('/')[0])\")\n",
    "  FOLDER=$(python -c \"print('/'.join('$NOTEBOOK_DIR'.split('//')[1].split('/')[1:-1]))\")\n",
    "\n",
    "  echo \"bucket '$BUCKET' folder '$FOLDER'\"\n",
    "\n",
    "  cd /mnt\n",
    "  git clone https://github.com/s3fs-fuse/s3fs-fuse.git\n",
    "  cd s3fs-fuse/\n",
    "  ls -alrt\n",
    "  ./autogen.sh\n",
    "  PKG_CONFIG=/usr/bin/pkg-config ./configure\n",
    "  #./configure\n",
    "  make\n",
    "  sudo make install\n",
    "  sudo su -c 'echo user_allow_other >> /etc/fuse.conf'\n",
    "  mkdir -p /mnt/s3fs-cache\n",
    "  mkdir -p /mnt/$BUCKET\n",
    "  /usr/local/bin/s3fs -o allow_other -o iam_role=auto -o umask=0 -o url=https://s3.amazonaws.com  -o no_check_certificate -o enable_noobj_cache -o use_cache=/mnt/s3fs-cache $BUCKET /mnt/$BUCKET\n",
    "\n",
    "  ### Install Jupyter Notebook with conda and configure it.\n",
    "  echo \"installing python libs in master\"\n",
    "  # install\n",
    "  conda install jupyter\n",
    "\n",
    "  # install visualization libs\n",
    "  conda install matplotlib plotly bokeh\n",
    "\n",
    "  # install scikit-learn stable version\n",
    "  #conda install --channel scikit-learn-contrib scikit-learn==0.18\n",
    "\n",
    "  # jupyter configs\n",
    "  mkdir -p ~/.jupyter\n",
    "  touch ls ~/.jupyter/jupyter_notebook_config.py\n",
    "  HASHED_PASSWORD=$(python -c \"from notebook.auth import passwd; print(passwd('$JUPYTER_PASSWORD'))\")\n",
    "  echo \"c.NotebookApp.password = u'$HASHED_PASSWORD'\" >> ~/.jupyter/jupyter_notebook_config.py\n",
    "  echo \"c.NotebookApp.open_browser = False\" >> ~/.jupyter/jupyter_notebook_config.py\n",
    "  echo \"c.NotebookApp.ip = '*'\" >> ~/.jupyter/jupyter_notebook_config.py\n",
    "  echo \"c.NotebookApp.notebook_dir = '/mnt/$BUCKET/$FOLDER'\" >> ~/.jupyter/jupyter_notebook_config.py\n",
    "  echo \"c.ContentsManager.checkpoints_kwargs = {'root_dir': '.checkpoints'}\" >> ~/.jupyter/jupyter_notebook_config.py\n",
    "\n",
    "  ### Setup Jupyter deamon and launch it\n",
    "  cd ~\n",
    "  echo \"Creating Jupyter Daemon\"\n",
    "\n",
    "  sudo cat <<EOF > /home/hadoop/jupyter.conf\n",
    "description \"Jupyter\"\n",
    "\n",
    "start on runlevel [2345]\n",
    "stop on runlevel [016]\n",
    "\n",
    "respawn\n",
    "respawn limit 0 10\n",
    "\n",
    "chdir /mnt/$BUCKET/$FOLDER\n",
    "\n",
    "script\n",
    "  sudo su - hadoop > /var/log/jupyter.log 2>&1 <<BASH_SCRIPT\n",
    "        export PYSPARK_DRIVER_PYTHON=\"/home/hadoop/conda/bin/jupyter\"\n",
    "        export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --log-level=INFO\"\n",
    "        export PYSPARK_PYTHON=/home/hadoop/conda/bin/python3.5\n",
    "        export JAVA_HOME=\"/etc/alternatives/jre\"\n",
    "        pyspark\n",
    "  BASH_SCRIPT\n",
    "\n",
    "end script\n",
    "EOF\n",
    "\n",
    "  sudo mv /home/hadoop/jupyter.conf /etc/init/\n",
    "  sudo chown root:root /etc/init/jupyter.conf\n",
    "\n",
    "  sudo initctl reload-configuration\n",
    "\n",
    "  # start jupyter daemon\n",
    "  echo \"Starting Jupyter Daemon\"\n",
    "  sudo initctl start jupyter\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Launch an EMR cluster</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws emr create-cluster \\\n",
    "    --name \"Spark_Cluster\" \\\n",
    "    --release-label emr-5.8.0 \\\n",
    "    --applications Name=Spark Name=Zeppelin \\\n",
    "    --log-uri s3://YOUR_BUCKET/logs/ \\\n",
    "    --service-role emr-default-role \\\n",
    "    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.large InstanceGroupType=CORE,InstanceCount=1,InstanceType=m4.large \\\n",
    "    --ec2-attributes InstanceProfile=emr-default-ec2-role,KeyName=YOUR_KEY,SubnetId=\"YOUR_SUBNET\" \\\n",
    "    --bootstrap-action Name=\"Install Jupyter notebook\",Path=\"s3://YOUR_BUCKET/scripts/install-jupyter.sh\",Args=[\"jupyter\",\"s3://YOUR_BUCKET/jupyter/\"] \\\n",
    "    --configurations file:///YOUR_PATH/config/hdfs-config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Check a state of the launching cluster</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws emr describe-cluster --cluster-id YOUR_CLUSTER_ID --query \"Cluster.Status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Set up an SSH tunnel using dynamic port forwarding (run in your terminal)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo aws emr socks --cluster-id YOUR_CLUSTER_ID --key-pair-file /YOUR_PATH/your_private_key.pem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Print out an internal host name of the master node</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws emr list-instances \\\n",
    "        --cluster-id YOUR_CLUSTER_ID \\\n",
    "        --instance-group-types \"MASTER\" \\\n",
    "        --query \"Instances[0].PrivateDnsName\" \\\n",
    "        --output text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Switch to your browser with the foxyproxy exstension and enter the internal host name and a port to open the Jupyter dashboard</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For example,</p>\n",
    "<p class=\"code-block code-font\">ip-10-0-1-11.eu-west-1.compute.internal:<span class=\"code-key\">8888</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Upload the files in the <span class=\"code-font\">data/</span> directory to S3. Open the <span class=\"code-font\">Spark_Dataframe_Basics.ipynb</span> notebook on the EMR Cluster through the Jupyter dashboard</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Complete tasks in the notebook</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Terminate the cluster after completing the class tasks</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr terminate-clusters --cluster-ids YOUR_CLUSTER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Make sure all clusters are terminated</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr list-clusters --active "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "<div style=\"display:table; width:100%; padding-top:10px; padding-bottom:10px; border-bottom:1px solid lightgrey\">\n",
    "    <div style=\"display:table-row\">\n",
    "        <div style=\"display:table-cell; width:80%; font-size:14pt; font-weight:bold\">2. References</div>\n",
    "    \t<div style=\"display:table-cell; width:20%; text-align:center; background-color:whitesmoke; border:1px solid lightgrey\"><a href=\"#0\">To content</a></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://bytes.babbel.com/en/articles/2017-07-04-spark-with-jupyter-inside-vpc.html\">Launch an AWS EMR cluster with Pyspark and Jupyter Notebook inside a VPC</a><br>\n",
    "<a href=\"http://www.exegetic.biz/blog/2017/08/using-aws-cli/\">Driving AWS from the Command Line</a><br>\n",
    "<a href=\"https://jupyterhub.readthedocs.io/en/latest/\">JupyterHub</a><br>\n",
    "<a href=\"https://github.com/jupyterhub/jupyterhub-tutorial\">Getting Started with JupyterHub tutorial</a><br>\n",
    "<a href=\"https://medium.com/@muppal/aws-emr-jupyter-spark-2-x-7da54dc4bfc8\">AWS EMR+ Jupyter + spark 2.x</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
