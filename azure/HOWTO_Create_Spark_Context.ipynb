{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:18pt; padding-top:20px; text-align:center\"><b>Подключение библиотек и создание </b> <span style=\"font-weight:bold; color:green\">Spark Context</span></div><hr>\n",
    "<div style=\"text-align:right;\">Папулин С.Ю. <span style=\"font-style: italic;font-weight: bold;\">(papulin_hse@mail.ru)</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Исходники для подключения библиотек и создание Spark Context</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/ZEMUSHKA/lsml_hse/tree/master/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Добавление директории со спарком</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def setup_pyspark_env(version=\"2.1.0\"):\n",
    "    if version == \"2.1.0\":\n",
    "        os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2.1\"\n",
    "        sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "        sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.10.4-src.zip'))\n",
    "    elif version == \"1.6.2\":\n",
    "        os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark-client\"\n",
    "        sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "        sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.9-src.zip'))\n",
    "    else:\n",
    "        raise Exception(\"Version not supported!\")\n",
    "        \n",
    "setup_pyspark_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Подключение модуля PySpark</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Создание Spark Context для запуска приложений</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Настраиваемые параметры: \n",
    "# http://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_spark_conf(parallelism, addPythonMemoryOverhead, nodesAlive,\n",
    "                   executorsPerNode, memoryPerExecutor):\n",
    "    executorInstances = nodesAlive * executorsPerNode - 1  # One for Application Master\n",
    "    executorMemoryOverheadMb = max(384, int(memoryPerExecutor * 0.10) + 1)  # default Spark behavior\n",
    "    if addPythonMemoryOverhead:\n",
    "        # python eats the same amount, add to overhead!\n",
    "        executorMemoryOverheadMb = int(memoryPerExecutor * 0.5)\n",
    "    executorMemoryMb = memoryPerExecutor - executorMemoryOverheadMb\n",
    "    conf = (\n",
    "        pyspark.SparkConf()\n",
    "        .set(\"spark.executor.memory\", \"{0}m\".format(executorMemoryMb))\n",
    "        .set(\"spark.driver.memory\", \"{0}m\".format(executorMemoryMb))\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", executorMemoryOverheadMb)\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", executorMemoryOverheadMb)\n",
    "        .set(\"spark.python.worker.memory\", \"{0}m\".format(int(executorMemoryOverheadMb * 0.8)))  # 10 % of memory is for other stuff\n",
    "        .set(\"spark.executor.instances\", executorInstances)\n",
    "        .set(\"spark.default.parallelism\", parallelism)\n",
    "    )\n",
    "    return conf\n",
    "\n",
    "\n",
    "def get_spark_context(master=\"yarn-client\", appName=\"Jupyter Notebook\",\n",
    "                      parallelism=300, addPythonMemoryOverhead=True, nodesAlive=3,\n",
    "                      executorsPerNode=4, memoryPerExecutor=6144):\n",
    "    sc = pyspark.SparkContext(\n",
    "        master=master,\n",
    "        appName=appName,\n",
    "        conf=get_spark_conf(parallelism, addPythonMemoryOverhead, nodesAlive, executorsPerNode, memoryPerExecutor)\n",
    "    )\n",
    "    print \"Ambari - http://10.0.1.21:8080\"\n",
    "    print \"All Applications - http://10.0.1.23:8088/cluster\"\n",
    "    return sc\n",
    "\n",
    "sc = get_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Остановка Spark Context и завершение приложения</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
